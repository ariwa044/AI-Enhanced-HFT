{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38ed3147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('Libraries loaded successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcede42e",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4638d262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded: 95764 rows\n",
      "Columns: ['Time', 'HA_Open', 'HA_High', 'HA_Low', 'HA_Close', 'Volume']\n",
      "Date range: 2023-01-01 00:00:00 to 2025-09-24 18:15:00\n",
      "\n",
      "First 5 rows:\n",
      "                  Time       HA_Open   HA_High    HA_Low    HA_Close  Volume\n",
      "0  2023-01-01 00:00:00  16520.510000  16529.87  16508.58  16519.8675       0\n",
      "1  2023-01-01 00:15:00  16520.188750  16530.87  16506.51  16519.8000       0\n",
      "2  2023-01-01 00:30:00  16519.994375  16533.31  16502.55  16518.7100       0\n",
      "3  2023-01-01 00:45:00  16519.352188  16521.36  16505.79  16512.3350       0\n",
      "4  2023-01-01 01:00:00  16515.843594  16530.62  16511.78  16520.6325       0\n"
     ]
    }
   ],
   "source": [
    "# Load Heiken Ashi data\n",
    "df = pd.read_csv('BTCUSD_15m_HA_data.csv')\n",
    "\n",
    "print(f'Data loaded: {len(df)} rows')\n",
    "print(f'Columns: {df.columns.tolist()}')\n",
    "print(f\"Date range: {df['Time'].min()} to {df['Time'].max()}\")\n",
    "print(f'\\nFirst 5 rows:')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac756783",
   "metadata": {},
   "source": [
    "## 3. Calculate Heiken Ashi Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b028d89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HA metrics calculated\n"
     ]
    }
   ],
   "source": [
    "# HA body size\n",
    "df['HA_Body'] = abs(df['HA_Close'] - df['HA_Open'])\n",
    "\n",
    "# HA range\n",
    "df['HA_Range'] = df['HA_High'] - df['HA_Low']\n",
    "\n",
    "# HA close change\n",
    "df['HA_Close_Change'] = df['HA_Close'].diff()\n",
    "\n",
    "# HA momentum\n",
    "df['HA_Momentum'] = df['HA_Close'].diff(3)\n",
    "\n",
    "# HA volatility\n",
    "df['HA_Volatility'] = df['HA_Range'].rolling(5).std()\n",
    "\n",
    "print('HA metrics calculated')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8215a1fa",
   "metadata": {},
   "source": [
    "## 4. K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20586721",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Fit K-means with 3 clusters\u001b[39;00m\n\u001b[1;32m     12\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m clusters \u001b[38;5;241m=\u001b[39m \u001b[43mkmeans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclose_prices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Current cluster\u001b[39;00m\n\u001b[1;32m     16\u001b[0m current_cluster \u001b[38;5;241m=\u001b[39m clusters[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/sklearn/cluster/_kmeans.py:1069\u001b[0m, in \u001b[0;36m_BaseKMeans.fit_predict\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute cluster centers and predict cluster index for each sample.\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \n\u001b[1;32m   1049\u001b[0m \u001b[38;5;124;03m    Convenience method; equivalent to calling fit(X) followed by\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;124;03m        Index of the cluster each sample belongs to.\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1069\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlabels_\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/sklearn/base.py:1147\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m-> 1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mconfig_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_parameter_validation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefer_skip_nested_validation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mglobal_skip_validation\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/contextlib.py:301\u001b[0m, in \u001b[0;36mcontextmanager.<locals>.helper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mhelper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m--> 301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_GeneratorContextManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/contextlib.py:108\u001b[0m, in \u001b[0;36m_GeneratorContextManagerBase.__init__\u001b[0;34m(self, func, args, kwds)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds \u001b[38;5;241m=\u001b[39m func, args, kwds\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Issue 19330: ensure context manager instances have good docstrings\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m__doc__\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Use 252 bars for clustering\n",
    "window = 252\n",
    "n_bars = len(df)\n",
    "\n",
    "# Pre-allocate numpy arrays (much faster than .loc in a loop)\n",
    "clusters = np.full(n_bars, np.nan)\n",
    "densities = np.full(n_bars, np.nan)\n",
    "\n",
    "# Cache close prices as numpy array (avoid repeated .iloc conversions)\n",
    "close_prices = df['HA_Close'].values.reshape(-1, 1)\n",
    "\n",
    "for i in range(window, n_bars):\n",
    "    # Get rolling window (numpy slicing is faster)\n",
    "    window_prices = close_prices[i-window:i]\n",
    "    \n",
    "    # Fit K-means with 3 clusters\n",
    "    # Optimized parameters: n_init=2 (instead of 10), max_iter limit\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=3,\n",
    "        n_init=2,           # Reduced from 10 (still stable)\n",
    "        max_iter=100,       # Add iteration limit for convergence\n",
    "        random_state=42,\n",
    "        tol=1e-3            # Slightly relax tolerance\n",
    "    )\n",
    "    cluster_labels = kmeans.fit_predict(window_prices)\n",
    "    \n",
    "    # Current cluster\n",
    "    current_cluster = cluster_labels[-1]\n",
    "    clusters[i] = current_cluster\n",
    "    \n",
    "    # Cluster density (pure numpy is faster)\n",
    "    cluster_count = np.sum(cluster_labels == current_cluster)\n",
    "    densities[i] = (cluster_count / window) * 100\n",
    "\n",
    "# Assign back to dataframe\n",
    "df['Cluster'] = clusters\n",
    "df['Cluster_Density'] = densities\n",
    "\n",
    "print('K-Means clustering complete')\n",
    "print(f'Processed {n_bars - window} rolling windows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592d8b49",
   "metadata": {},
   "source": [
    "## 5. Consecutive Bars Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f1d155",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Consecutive_Up'] = 0\n",
    "df['Consecutive_Down'] = 0\n",
    "\n",
    "up_count = 0\n",
    "down_count = 0\n",
    "\n",
    "for i in range(1, len(df)):\n",
    "    if df['HA_Close'].iloc[i] > df['HA_Close'].iloc[i-1]:\n",
    "        up_count += 1\n",
    "        down_count = 0\n",
    "    elif df['HA_Close'].iloc[i] < df['HA_Close'].iloc[i-1]:\n",
    "        down_count += 1\n",
    "        up_count = 0\n",
    "    else:\n",
    "        up_count = 0\n",
    "        down_count = 0\n",
    "    \n",
    "    df.loc[i, 'Consecutive_Up'] = up_count\n",
    "    df.loc[i, 'Consecutive_Down'] = down_count\n",
    "\n",
    "print('Consecutive bars calculated')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bfc7e2",
   "metadata": {},
   "source": [
    "## 6. Volume Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae45078",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Volume_Change'] = df['Volume'].pct_change()\n",
    "df['Volume_MA_5'] = df['Volume'].rolling(5).mean()\n",
    "df['Volume_Ratio'] = df['Volume'] / df['Volume_MA_5']\n",
    "\n",
    "print('Volume features calculated')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a357e835",
   "metadata": {},
   "source": [
    "## 7. Create Target Label (10 bars ahead for day trading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4ce5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target: Predict price direction 10 bars ahead (~2.5 hours on M15)\n",
    "# This converts from scalping (1-bar) to day trading (2.5+ hours)\n",
    "shift_bars = 10  # 10 bars * 15min = 2.5 hours\n",
    "\n",
    "df['Target'] = 0\n",
    "for i in range(len(df) - shift_bars):\n",
    "    if df['HA_Close'].iloc[i + shift_bars] > df['HA_Close'].iloc[i]:\n",
    "        df.loc[i, 'Target'] = 1  # Bullish (price higher in 2.5 hours)\n",
    "    else:\n",
    "        df.loc[i, 'Target'] = -1  # Bearish (price lower in 2.5 hours)\n",
    "\n",
    "# Remove last 10 rows (no target, not enough bars ahead)\n",
    "df = df[:-shift_bars]\n",
    "\n",
    "print(f'Target distribution (predicting {shift_bars} bars / 2.5 hours ahead):')\n",
    "print(df['Target'].value_counts())\n",
    "print(f\"\\nBullish: {(df['Target'] == 1).sum()} ({(df['Target'] == 1).sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"Bearish: {(df['Target'] == -1).sum()} ({(df['Target'] == -1).sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"\\nThis means:\")\n",
    "print(f\"  - Model predicts price direction 2.5 hours into the future\")\n",
    "print(f\"  - Average trade holding time will be ~2.5 hours (not 1 bar)\")\n",
    "print(f\"  - Day trading style, not scalping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544a32bb",
   "metadata": {},
   "source": [
    "## 8. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ab5ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    'HA_Open', 'HA_High', 'HA_Low', 'HA_Close',  # Price\n",
    "    'HA_Body', 'HA_Range', 'HA_Close_Change',    # HA metrics\n",
    "    'HA_Momentum', 'HA_Volatility',              # Momentum\n",
    "    'Cluster_Density',                           # Clustering\n",
    "    'Consecutive_Up', 'Consecutive_Down',       # Patterns\n",
    "    'Volume', 'Volume_Change', 'Volume_Ratio'   # Volume\n",
    "]\n",
    "\n",
    "df = df.fillna(0)\n",
    "X = df[feature_columns].values\n",
    "y = df['Target'].values\n",
    "\n",
    "print(f'Features shape: {X.shape}')\n",
    "print(f'Target shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdae9e87",
   "metadata": {},
   "source": [
    "## 9. Standardize Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ff2305",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "joblib.dump(scaler, 'scaler_randomforest_ha15m.save')\n",
    "print('Scaler saved: scaler_randomforest_ha15m.save')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b325d006",
   "metadata": {},
   "source": [
    "## 10. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0b3719",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, shuffle=False\n",
    ")\n",
    "\n",
    "print(f'Training set: {X_train.shape}')\n",
    "print(f'Test set: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595147ad",
   "metadata": {},
   "source": [
    "## 11. Train Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf207fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=20,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "print('Training Random Forest...')\n",
    "model.fit(X_train, y_train)\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d866453f",
   "metadata": {},
   "source": [
    "## 12. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a6b4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "\n",
    "print(f'Random Forest Model Performance:')\n",
    "print(f'  Accuracy:  {accuracy:.4f}')\n",
    "print(f'  Precision: {precision:.4f}')\n",
    "print(f'  Recall:    {recall:.4f}')\n",
    "print(f'  F1-Score:  {f1:.4f}')\n",
    "print(f'\\nConfusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c1ca24",
   "metadata": {},
   "source": [
    "## 13. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5409b2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print('Feature Importance Ranking:')\n",
    "for i in range(min(10, len(feature_columns))):\n",
    "    print(f'{i+1}. {feature_columns[indices[i]]}: {importances[indices[i]]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e55961b",
   "metadata": {},
   "source": [
    "## 14. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9a24f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model, 'randomforest_ha15m_trend_model.pkl')\n",
    "print('Model saved: randomforest_ha15m_trend_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0052a3d7",
   "metadata": {},
   "source": [
    "## 15. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55beda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for entire dataset\n",
    "y_pred_all = model.predict(X_scaled)\n",
    "\n",
    "print(f'Total predictions: {len(y_pred_all)}')\n",
    "print(f'Bullish: {(y_pred_all == 1).sum()}')\n",
    "print(f'Bearish: {(y_pred_all == -1).sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3349d2",
   "metadata": {},
   "source": [
    "## 16. Save Forecast CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30a1c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_df = df[['Time']].copy()\n",
    "forecast_df['RF_Prediction'] = y_pred_all\n",
    "\n",
    "forecast_df.to_csv('randomforest_ha15m_forecast.csv', index=False)\n",
    "\n",
    "print(f'Forecast saved: randomforest_ha15m_forecast.csv')\n",
    "print(forecast_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2945b1b",
   "metadata": {},
   "source": [
    "## 17. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49436afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*50)\n",
    "print('RANDOM FOREST TRAINING SUMMARY')\n",
    "print('='*50)\n",
    "print(f'\\nModel Type: Random Forest (200 trees, max_depth=20)')\n",
    "print(f'Total Features: {len(feature_columns)}')\n",
    "print(f'Total Samples: {len(df)}')\n",
    "print(f'Training Samples: {len(X_train)}')\n",
    "print(f'Test Samples: {len(X_test)}')\n",
    "print(f'\\nTest Set Performance:')\n",
    "print(f'  Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)')\n",
    "print(f'  Precision: {precision:.4f}')\n",
    "print(f'  Recall:    {recall:.4f}')\n",
    "print(f'  F1-Score:  {f1:.4f}')\n",
    "print(f'\\nOutput Files:')\n",
    "print(f'  1. randomforest_ha15m_trend_model.pkl')\n",
    "print(f'  2. scaler_randomforest_ha15m.save')\n",
    "print(f'  3. randomforest_ha15m_forecast.csv')\n",
    "print('='*50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
