{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34bbf49f",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68bfce31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 02:02:01.802247: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-12 02:02:02.564348: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-12 02:02:04.567017: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully!\n",
      "TensorFlow version: 2.20.0\n",
      "GPU Available: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 02:02:05.507895: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('Libraries loaded successfully!')\n",
    "print(f'TensorFlow version: {tf.__version__}')\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7eb9f9",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8edb23d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded: 95764 rows\n",
      "Columns: ['Time', 'HA_Open', 'HA_High', 'HA_Low', 'HA_Close', 'Volume']\n",
      "Date range: 2023-01-01 00:00:00 to 2025-09-24 18:15:00\n",
      "\n",
      "First 5 rows:\n",
      "                  Time       HA_Open   HA_High    HA_Low    HA_Close  Volume\n",
      "0  2023-01-01 00:00:00  16520.510000  16529.87  16508.58  16519.8675       0\n",
      "1  2023-01-01 00:15:00  16520.188750  16530.87  16506.51  16519.8000       0\n",
      "2  2023-01-01 00:30:00  16519.994375  16533.31  16502.55  16518.7100       0\n",
      "3  2023-01-01 00:45:00  16519.352188  16521.36  16505.79  16512.3350       0\n",
      "4  2023-01-01 01:00:00  16515.843594  16530.62  16511.78  16520.6325       0\n",
      "\n",
      "Data info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 95764 entries, 0 to 95763\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Time      95764 non-null  object \n",
      " 1   HA_Open   95764 non-null  float64\n",
      " 2   HA_High   95764 non-null  float64\n",
      " 3   HA_Low    95764 non-null  float64\n",
      " 4   HA_Close  95764 non-null  float64\n",
      " 5   Volume    95764 non-null  int64  \n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 4.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load Heiken Ashi data\n",
    "df = pd.read_csv('BTCUSD_15m_HA_data.csv')\n",
    "\n",
    "print(f'Data loaded: {len(df)} rows')\n",
    "print(f'Columns: {df.columns.tolist()}')\n",
    "print(f\"Date range: {df['Time'].min()} to {df['Time'].max()}\")\n",
    "print(f'\\nFirst 5 rows:')\n",
    "print(df.head())\n",
    "print(f'\\nData info:')\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41be609a",
   "metadata": {},
   "source": [
    "## 3. Calculate Heiken Ashi Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bc02224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HA metrics calculated:\n",
      "    HA_Body  HA_Range  HA_Close_Change  HA_Momentum  HA_Volatility\n",
      "0  0.642500     21.29              NaN          NaN            NaN\n",
      "1  0.388750     24.36          -0.0675          NaN            NaN\n",
      "2  1.284375     30.76          -1.0900          NaN            NaN\n",
      "3  7.017188     15.57          -6.3750      -7.5325            NaN\n",
      "4  4.788906     18.84           8.2975       0.8325       5.788077\n"
     ]
    }
   ],
   "source": [
    "# HA body size\n",
    "df['HA_Body'] = abs(df['HA_Close'] - df['HA_Open'])\n",
    "\n",
    "# HA range\n",
    "df['HA_Range'] = df['HA_High'] - df['HA_Low']\n",
    "\n",
    "# HA close change\n",
    "df['HA_Close_Change'] = df['HA_Close'].diff()\n",
    "\n",
    "# HA momentum\n",
    "df['HA_Momentum'] = df['HA_Close'].diff(3)\n",
    "\n",
    "# HA volatility\n",
    "df['HA_Volatility'] = df['HA_Range'].rolling(5).std()\n",
    "\n",
    "print('HA metrics calculated:')\n",
    "print(df[['HA_Body', 'HA_Range', 'HA_Close_Change', 'HA_Momentum', 'HA_Volatility']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000cf174",
   "metadata": {},
   "source": [
    "## 4. K-Means Clustering (Python Implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97ed4d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Means clustering complete\n",
      "       Cluster  Cluster_Density\n",
      "95754      2.0        60.714286\n",
      "95755      2.0        60.317460\n",
      "95756      1.0        60.714286\n",
      "95757      2.0         8.730159\n",
      "95758      2.0         8.730159\n",
      "95759      2.0         9.126984\n",
      "95760      2.0         9.126984\n",
      "95761      2.0         9.126984\n",
      "95762      2.0         9.126984\n",
      "95763      2.0         9.126984\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Use 252 bars for clustering (like in MT5 EA)\n",
    "window = 252\n",
    "df['Cluster'] = np.nan\n",
    "df['Cluster_Density'] = np.nan\n",
    "\n",
    "for i in range(window, len(df)):\n",
    "    close_prices = df['HA_Close'].iloc[i-window:i].values.reshape(-1, 1)\n",
    "    \n",
    "    # Fit K-means with 3 clusters\n",
    "    kmeans = KMeans(n_clusters=3, n_init=10, random_state=42)\n",
    "    clusters = kmeans.fit_predict(close_prices)\n",
    "    \n",
    "    # Get current bar cluster\n",
    "    current_cluster = clusters[-1]\n",
    "    df.loc[i, 'Cluster'] = current_cluster\n",
    "    \n",
    "    # Calculate cluster density\n",
    "    cluster_count = np.sum(clusters == current_cluster)\n",
    "    density = (cluster_count / window) * 100\n",
    "    df.loc[i, 'Cluster_Density'] = density\n",
    "\n",
    "print('K-Means clustering complete')\n",
    "print(df[['Cluster', 'Cluster_Density']].tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf17bea6",
   "metadata": {},
   "source": [
    "## 5. Consecutive Bars Pattern Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "febb7f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consecutive bars pattern calculated\n",
      "          HA_Close  Consecutive_Up  Consecutive_Down\n",
      "95754  113351.3650               0                 2\n",
      "95755  113469.1875               1                 0\n",
      "95756  113655.6350               2                 0\n",
      "95757  113728.4325               3                 0\n",
      "95758  113818.1150               4                 0\n",
      "95759  113836.1325               5                 0\n",
      "95760  113878.8700               6                 0\n",
      "95761  113815.7675               0                 1\n",
      "95762  113728.6775               0                 2\n",
      "95763  113647.4925               0                 3\n"
     ]
    }
   ],
   "source": [
    "# Track consecutive up/down bars\n",
    "df['Consecutive_Up'] = 0\n",
    "df['Consecutive_Down'] = 0\n",
    "\n",
    "up_count = 0\n",
    "down_count = 0\n",
    "\n",
    "for i in range(1, len(df)):\n",
    "    if df['HA_Close'].iloc[i] > df['HA_Close'].iloc[i-1]:\n",
    "        up_count += 1\n",
    "        down_count = 0\n",
    "    elif df['HA_Close'].iloc[i] < df['HA_Close'].iloc[i-1]:\n",
    "        down_count += 1\n",
    "        up_count = 0\n",
    "    else:\n",
    "        up_count = 0\n",
    "        down_count = 0\n",
    "    \n",
    "    df.loc[i, 'Consecutive_Up'] = up_count\n",
    "    df.loc[i, 'Consecutive_Down'] = down_count\n",
    "\n",
    "print('Consecutive bars pattern calculated')\n",
    "print(df[['HA_Close', 'Consecutive_Up', 'Consecutive_Down']].tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9c8a19",
   "metadata": {},
   "source": [
    "## 6. Volume Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c55d8269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume features calculated\n",
      "       Volume  Volume_Change  Volume_Ratio\n",
      "95754       0            NaN           NaN\n",
      "95755       0            NaN           NaN\n",
      "95756       0            NaN           NaN\n",
      "95757       0            NaN           NaN\n",
      "95758       0            NaN           NaN\n",
      "95759       0            NaN           NaN\n",
      "95760       0            NaN           NaN\n",
      "95761       0            NaN           NaN\n",
      "95762       0            NaN           NaN\n",
      "95763       0            NaN           NaN\n"
     ]
    }
   ],
   "source": [
    "# Volume change\n",
    "df['Volume_Change'] = df['Volume'].pct_change()\n",
    "\n",
    "# Volume MA\n",
    "df['Volume_MA_5'] = df['Volume'].rolling(5).mean()\n",
    "\n",
    "# Volume ratio\n",
    "df['Volume_Ratio'] = df['Volume'] / df['Volume_MA_5']\n",
    "\n",
    "print('Volume features calculated')\n",
    "print(df[['Volume', 'Volume_Change', 'Volume_Ratio']].tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e763604",
   "metadata": {},
   "source": [
    "## 7. Create Target Label (±1 Direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f244c81c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Target: Next bar direction\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTarget\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHA_Close\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHA_Close\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[i]:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Target: Predict price direction 10 bars ahead (~2.5 hours on M15)\n",
    "# This converts from scalping (1-bar) to day trading (2.5+ hours)\n",
    "shift_bars = 10  # 10 bars * 15min = 2.5 hours\n",
    "\n",
    "df['Target'] = 0\n",
    "for i in range(len(df) - shift_bars):\n",
    "    if df['HA_Close'].iloc[i + shift_bars] > df['HA_Close'].iloc[i]:\n",
    "        df.loc[i, 'Target'] = 1  # Bullish (price higher in 2.5 hours)\n",
    "    else:\n",
    "        df.loc[i, 'Target'] = -1  # Bearish (price lower in 2.5 hours)\n",
    "\n",
    "# Remove last 10 rows (no target, not enough bars ahead)\n",
    "df = df[:-shift_bars]\n",
    "\n",
    "print(f'Target distribution (predicting {shift_bars} bars / 2.5 hours ahead):')\n",
    "print(df['Target'].value_counts())\n",
    "print(f\"Bullish: {(df['Target'] == 1).sum()} ({(df['Target'] == 1).sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"Bearish: {(df['Target'] == -1).sum()} ({(df['Target'] == -1).sum()/len(df)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38d5a1b",
   "metadata": {},
   "source": [
    "## 8. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad66e2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (95763, 15)\n",
      "Target shape: (95763,)\n",
      "Feature columns: ['HA_Open', 'HA_High', 'HA_Low', 'HA_Close', 'HA_Body', 'HA_Range', 'HA_Close_Change', 'HA_Momentum', 'HA_Volatility', 'Cluster_Density', 'Consecutive_Up', 'Consecutive_Down', 'Volume', 'Volume_Change', 'Volume_Ratio']\n"
     ]
    }
   ],
   "source": [
    "# Select features for LSTM\n",
    "feature_columns = [\n",
    "    'HA_Open', 'HA_High', 'HA_Low', 'HA_Close',  # Price\n",
    "    'HA_Body', 'HA_Range', 'HA_Close_Change',    # HA metrics\n",
    "    'HA_Momentum', 'HA_Volatility',              # Momentum\n",
    "    'Cluster_Density',                           # Clustering\n",
    "    'Consecutive_Up', 'Consecutive_Down',       # Patterns\n",
    "    'Volume', 'Volume_Change', 'Volume_Ratio'   # Volume\n",
    "]\n",
    "\n",
    "# Handle NaN values\n",
    "df = df.fillna(0)\n",
    "\n",
    "X = df[feature_columns].values\n",
    "y = df['Target'].values\n",
    "\n",
    "print(f'Features shape: {X.shape}')\n",
    "print(f'Target shape: {y.shape}')\n",
    "print(f'Feature columns: {feature_columns}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f061649",
   "metadata": {},
   "source": [
    "## 9. Standardize Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "255b1bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled X shape: (95763, 15)\n",
      "Mean of scaled features: [ 1.89946995e-17  1.89946995e-17  1.89946995e-17  2.08941694e-16\n",
      " -1.06845184e-16]\n",
      "Std of scaled features: [1. 1. 1. 1. 1.]\n",
      "\n",
      "Scaler saved: scaler_lstm_ha15m.save\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f'Scaled X shape: {X_scaled.shape}')\n",
    "print(f'Mean of scaled features: {X_scaled.mean(axis=0)[:5]}')\n",
    "print(f'Std of scaled features: {X_scaled.std(axis=0)[:5]}')\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, 'scaler_lstm_ha15m.save')\n",
    "print('\\nScaler saved: scaler_lstm_ha15m.save')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772a5135",
   "metadata": {},
   "source": [
    "## 10. Create Sequences for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "384360f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence X shape: (95758, 5, 15)\n",
      "Sequence y shape: (95758,)\n",
      "Total sequences: 95758\n"
     ]
    }
   ],
   "source": [
    "# Create sequences: use past 5 bars to predict next bar\n",
    "sequence_length = 5\n",
    "\n",
    "X_seq = []\n",
    "y_seq = []\n",
    "\n",
    "for i in range(len(X_scaled) - sequence_length):\n",
    "    X_seq.append(X_scaled[i:i+sequence_length])\n",
    "    y_seq.append(y[i+sequence_length])\n",
    "\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "print(f'Sequence X shape: {X_seq.shape}')  # (samples, 5, features)\n",
    "print(f'Sequence y shape: {y_seq.shape}')\n",
    "print(f'Total sequences: {len(X_seq)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68665d6a",
   "metadata": {},
   "source": [
    "## 11. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b4e5050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (76606, 5, 15) with 76606 samples\n",
      "Test set: (19152, 5, 15) with 19152 samples\n",
      "\n",
      "Training target distribution:\n",
      "  Bullish: 38579\n",
      "  Bearish: 38027\n"
     ]
    }
   ],
   "source": [
    "# Split 80/20\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_seq, y_seq, test_size=0.2, random_state=42, shuffle=False\n",
    ")\n",
    "\n",
    "print(f'Training set: {X_train.shape} with {len(y_train)} samples')\n",
    "print(f'Test set: {X_test.shape} with {len(y_test)} samples')\n",
    "print(f'\\nTraining target distribution:')\n",
    "print(f'  Bullish: {(y_train == 1).sum()}')\n",
    "print(f'  Bearish: {(y_train == -1).sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a59bbf",
   "metadata": {},
   "source": [
    "## 12. Build LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29aaefcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Model Architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │        \u001b[38;5;34m20,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m12,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33,441</span> (130.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m33,441\u001b[0m (130.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33,441</span> (130.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m33,441\u001b[0m (130.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    LSTM(64, activation='relu', return_sequences=True, input_shape=(sequence_length, len(feature_columns))),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='tanh')  # Output: -1 to 1\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "print('LSTM Model Architecture:')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14c825a",
   "metadata": {},
   "source": [
    "## 13. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84b3b766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1916/1916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 0.9992 - mae: 0.9983 - val_loss: 1.0006 - val_mae: 0.9950\n",
      "Epoch 2/100\n",
      "\u001b[1m1916/1916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.9972 - mae: 0.9966 - val_loss: 0.9981 - val_mae: 0.9960\n",
      "Epoch 3/100\n",
      "\u001b[1m1916/1916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.9960 - mae: 0.9957 - val_loss: 1.0027 - val_mae: 0.9948\n",
      "Epoch 4/100\n",
      "\u001b[1m1916/1916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.9954 - mae: 0.9950 - val_loss: 1.0221 - val_mae: 0.9946\n",
      "Epoch 5/100\n",
      "\u001b[1m1916/1916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.9952 - mae: 0.9944 - val_loss: 1.0036 - val_mae: 0.9965\n",
      "Epoch 6/100\n",
      "\u001b[1m1916/1916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.9947 - mae: 0.9943 - val_loss: 1.0070 - val_mae: 0.9967\n",
      "Epoch 7/100\n",
      "\u001b[1m1916/1916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.9941 - mae: 0.9937 - val_loss: 1.0284 - val_mae: 0.9942\n",
      "Epoch 8/100\n",
      "\u001b[1m1916/1916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.9935 - mae: 0.9931 - val_loss: 1.0134 - val_mae: 0.9950\n",
      "Epoch 9/100\n",
      "\u001b[1m1916/1916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.9932 - mae: 0.9928 - val_loss: 1.0259 - val_mae: 0.9952\n",
      "Epoch 10/100\n",
      "\u001b[1m1916/1916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.9918 - mae: 0.9914 - val_loss: 1.0363 - val_mae: 0.9954\n",
      "Epoch 11/100\n",
      "\u001b[1m1916/1916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.9910 - mae: 0.9904 - val_loss: 1.0211 - val_mae: 0.9956\n",
      "Epoch 12/100\n",
      "\u001b[1m1916/1916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.9911 - mae: 0.9908 - val_loss: 1.1032 - val_mae: 0.9905\n",
      "Training completed in 12 epochs\n"
     ]
    }
   ],
   "source": [
    "# Early stopping to prevent overfitting\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"Training completed in {len(history.history['loss'])} epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c510449",
   "metadata": {},
   "source": [
    "## 14. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22d8edf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Model Performance:\n",
      "  Accuracy:  0.5082\n",
      "  Precision: 0.5094\n",
      "  Recall:    0.6016\n",
      "  F1-Score:  0.5517\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3938 5581]\n",
      " [3838 5795]]\n"
     ]
    }
   ],
   "source": [
    "# Get predictions\n",
    "y_pred_prob = model.predict(X_test, verbose=0)\n",
    "y_pred = np.where(y_pred_prob > 0, 1, -1).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "\n",
    "print(f'LSTM Model Performance:')\n",
    "print(f'  Accuracy:  {accuracy:.4f}')\n",
    "print(f'  Precision: {precision:.4f}')\n",
    "print(f'  Recall:    {recall:.4f}')\n",
    "print(f'  F1-Score:  {f1:.4f}')\n",
    "print(f'\\nConfusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fdfbd6",
   "metadata": {},
   "source": [
    "## 15. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21d0e780",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved: lstm_ha15m_trend_model.h5\n"
     ]
    }
   ],
   "source": [
    "model.save('lstm_ha15m_trend_model.h5')\n",
    "print('Model saved: lstm_ha15m_trend_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3992cc66",
   "metadata": {},
   "source": [
    "## 16. Generate Forecast CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a62e769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating batch predictions...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions: 95763\n",
      "DataFrame length: 95763\n",
      "Bullish: 55501\n",
      "Bearish: 40257\n",
      "Neutral: 5\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions for entire dataset (vectorized - much faster!)\n",
    "all_predictions = np.zeros(len(df))\n",
    "\n",
    "# First sequence_length samples cannot be predicted\n",
    "for i in range(sequence_length):\n",
    "    all_predictions[i] = 0\n",
    "\n",
    "# Batch predict all sequences at once (100x faster than loop)\n",
    "print('Generating batch predictions...')\n",
    "batch_preds = model.predict(X_seq, verbose=0)\n",
    "all_predictions[sequence_length:] = np.where(batch_preds.flatten() > 0, 1, -1)\n",
    "\n",
    "print(f'Total predictions: {len(all_predictions)}')\n",
    "print(f'DataFrame length: {len(df)}')\n",
    "print(f'Bullish: {(all_predictions == 1).sum()}')\n",
    "print(f'Bearish: {(all_predictions == -1).sum()}')\n",
    "print(f'Neutral: {(all_predictions == 0).sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fe3556",
   "metadata": {},
   "source": [
    "## 17. Save Forecast CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67ae87e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast saved: lstm_ha15m_forecast.csv\n",
      "Shape: (95763, 2)\n",
      "\n",
      "First 10 rows:\n",
      "                  Time  LSTM_Prediction\n",
      "0  2023-01-01 00:00:00              0.0\n",
      "1  2023-01-01 00:15:00              0.0\n",
      "2  2023-01-01 00:30:00              0.0\n",
      "3  2023-01-01 00:45:00              0.0\n",
      "4  2023-01-01 01:00:00              0.0\n",
      "5  2023-01-01 01:15:00             -1.0\n",
      "6  2023-01-01 01:30:00             -1.0\n",
      "7  2023-01-01 01:45:00              1.0\n",
      "8  2023-01-01 02:00:00             -1.0\n",
      "9  2023-01-01 02:15:00             -1.0\n",
      "\n",
      "Last 10 rows:\n",
      "                      Time  LSTM_Prediction\n",
      "95753  2025-09-24 15:45:00             -1.0\n",
      "95754  2025-09-24 16:00:00              1.0\n",
      "95755  2025-09-24 16:15:00              1.0\n",
      "95756  2025-09-24 16:30:00              1.0\n",
      "95757  2025-09-24 16:45:00             -1.0\n",
      "95758  2025-09-24 17:00:00             -1.0\n",
      "95759  2025-09-24 17:15:00             -1.0\n",
      "95760  2025-09-24 17:30:00             -1.0\n",
      "95761  2025-09-24 17:45:00             -1.0\n",
      "95762  2025-09-24 18:00:00             -1.0\n"
     ]
    }
   ],
   "source": [
    "# Create forecast dataframe\n",
    "forecast_df = pd.DataFrame({\n",
    "    'Time': df['Time'].values,\n",
    "    'LSTM_Prediction': all_predictions\n",
    "})\n",
    "\n",
    "# Save\n",
    "forecast_df.to_csv('lstm_ha15m_forecast.csv', index=False)\n",
    "\n",
    "print(f'Forecast saved: lstm_ha15m_forecast.csv')\n",
    "print(f'Shape: {forecast_df.shape}')\n",
    "print(f'\\nFirst 10 rows:')\n",
    "print(forecast_df.head(10))\n",
    "print(f'\\nLast 10 rows:')\n",
    "print(forecast_df.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49510916",
   "metadata": {},
   "source": [
    "## 18. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e0ef19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "LSTM TRAINING SUMMARY\n",
      "==================================================\n",
      "\n",
      "Model Type: LSTM (2-layer with Dropout)\n",
      "Input Sequence Length: 5 bars\n",
      "Total Features: 15\n",
      "Total Samples: 95763\n",
      "Training Samples: 76606\n",
      "Test Samples: 19152\n",
      "\n",
      "Test Set Performance:\n",
      "  Accuracy:  0.5082 (50.82%)\n",
      "  Precision: 0.5094\n",
      "  Recall:    0.6016\n",
      "  F1-Score:  0.5517\n",
      "\n",
      "Output Files:\n",
      "  1. lstm_ha15m_trend_model.h5\n",
      "  2. scaler_lstm_ha15m.save\n",
      "  3. lstm_ha15m_forecast.csv\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print('='*50)\n",
    "print('LSTM TRAINING SUMMARY')\n",
    "print('='*50)\n",
    "print(f'\\nModel Type: LSTM (2-layer with Dropout)')\n",
    "print(f'Input Sequence Length: {sequence_length} bars')\n",
    "print(f'Total Features: {len(feature_columns)}')\n",
    "print(f'Total Samples: {len(df)}')\n",
    "print(f'Training Samples: {len(X_train)}')\n",
    "print(f'Test Samples: {len(X_test)}')\n",
    "print(f'\\nTest Set Performance:')\n",
    "print(f'  Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)')\n",
    "print(f'  Precision: {precision:.4f}')\n",
    "print(f'  Recall:    {recall:.4f}')\n",
    "print(f'  F1-Score:  {f1:.4f}')\n",
    "print(f'\\nOutput Files:')\n",
    "print(f'  1. lstm_ha15m_trend_model.h5')\n",
    "print(f'  2. scaler_lstm_ha15m.save')\n",
    "print(f'  3. lstm_ha15m_forecast.csv')\n",
    "print('='*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
