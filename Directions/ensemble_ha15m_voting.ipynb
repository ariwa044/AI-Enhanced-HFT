{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "784f06ce",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6b132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print('Libraries loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf9a219",
   "metadata": {},
   "source": [
    "## 2. Load Individual Model Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10825d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load forecasts from all three models\n",
    "lstm_forecast = pd.read_csv('lstm_ha15m_forecast.csv')\n",
    "rf_forecast = pd.read_csv('randomforest_ha15m_forecast.csv')\n",
    "xgb_forecast = pd.read_csv('xgboost_ha15m_forecast.csv')\n",
    "\n",
    "print(f'LSTM shape: {lstm_forecast.shape}')\n",
    "print(f'Random Forest shape: {rf_forecast.shape}')\n",
    "print(f'XGBoost shape: {xgb_forecast.shape}')\n",
    "\n",
    "print(f'\\nLSTM columns: {lstm_forecast.columns.tolist()}')\n",
    "print(f'RF columns: {rf_forecast.columns.tolist()}')\n",
    "print(f'XGB columns: {xgb_forecast.columns.tolist()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2640adc5",
   "metadata": {},
   "source": [
    "## 3. Merge Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458ba57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all forecasts\n",
    "ensemble_df = pd.DataFrame()\n",
    "ensemble_df['Time'] = lstm_forecast['Time']\n",
    "ensemble_df['LSTM_Pred'] = lstm_forecast.iloc[:, 1]  # Second column\n",
    "ensemble_df['RF_Pred'] = rf_forecast.iloc[:, 1]\n",
    "ensemble_df['XGB_Pred'] = xgb_forecast.iloc[:, 1]\n",
    "\n",
    "print(f'Ensemble dataframe shape: {ensemble_df.shape}')\n",
    "print(f'\\nFirst 10 rows:')\n",
    "print(ensemble_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1b2a82",
   "metadata": {},
   "source": [
    "## 4. Majority Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229b2a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble voting logic:\n",
    "# 2+ models predict 1 → signal = 1 (BULLISH)\n",
    "# 2+ models predict -1 → signal = -1 (BEARISH)\n",
    "# Split decision (1 votes each) → signal = 0 (NEUTRAL)\n",
    "\n",
    "def ensemble_vote(lstm, rf, xgb):\n",
    "    votes = [lstm, rf, xgb]\n",
    "    bullish = sum(1 for v in votes if v == 1)\n",
    "    bearish = sum(1 for v in votes if v == -1)\n",
    "    \n",
    "    if bullish >= 2:\n",
    "        return 1  # Consensus BULLISH\n",
    "    elif bearish >= 2:\n",
    "        return -1  # Consensus BEARISH\n",
    "    else:\n",
    "        return 0  # No consensus\n",
    "\n",
    "ensemble_df['Ensemble_Vote'] = ensemble_df.apply(\n",
    "    lambda row: ensemble_vote(row['LSTM_Pred'], row['RF_Pred'], row['XGB_Pred']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print('Ensemble voting complete!')\n",
    "print(f'\\nVote distribution:')\n",
    "print(ensemble_df['Ensemble_Vote'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d332f8b",
   "metadata": {},
   "source": [
    "## 5. Calculate Agreement Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab1c099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many models agree\n",
    "def count_agreement(lstm, rf, xgb):\n",
    "    votes = [lstm, rf, xgb]\n",
    "    agreement_count = 0\n",
    "    \n",
    "    # Count matching predictions\n",
    "    if lstm == rf:\n",
    "        agreement_count += 1\n",
    "    if lstm == xgb:\n",
    "        agreement_count += 1\n",
    "    if rf == xgb:\n",
    "        agreement_count += 1\n",
    "    \n",
    "    return agreement_count  # 0 to 3\n",
    "\n",
    "ensemble_df['Agreement_Count'] = ensemble_df.apply(\n",
    "    lambda row: count_agreement(row['LSTM_Pred'], row['RF_Pred'], row['XGB_Pred']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Agreement percentage\n",
    "ensemble_df['Consensus_Strength'] = (ensemble_df['Agreement_Count'] / 3) * 100\n",
    "\n",
    "print('Agreement metrics calculated')\n",
    "print(f'\\nAgreement distribution:')\n",
    "print(ensemble_df['Agreement_Count'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f545f9",
   "metadata": {},
   "source": [
    "## 6. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0e5e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bullish_signals = (ensemble_df['Ensemble_Vote'] == 1).sum()\n",
    "bearish_signals = (ensemble_df['Ensemble_Vote'] == -1).sum()\n",
    "neutral_signals = (ensemble_df['Ensemble_Vote'] == 0).sum()\n",
    "\n",
    "print(f'Ensemble Voting Results:')\n",
    "print(f'  Bullish signals: {bullish_signals} ({bullish_signals/len(ensemble_df)*100:.1f}%)')\n",
    "print(f'  Bearish signals: {bearish_signals} ({bearish_signals/len(ensemble_df)*100:.1f}%)')\n",
    "print(f'  Neutral signals: {neutral_signals} ({neutral_signals/len(ensemble_df)*100:.1f}%)')\n",
    "\n",
    "avg_consensus = ensemble_df['Consensus_Strength'].mean()\n",
    "print(f'\\nAverage Consensus Strength: {avg_consensus:.1f}%')\n",
    "print(f'  Strong consensus (100%): {(ensemble_df[\\\"Consensus_Strength\\\"] == 100).sum()}')\n",
    "print(f'  Moderate consensus (67%): {(ensemble_df[\\\"Consensus_Strength\\\"] == 66.67).sum()}')\n",
    "print(f'  Weak consensus (33%): {(ensemble_df[\\\"Consensus_Strength\\\"] == 33.33).sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66921e4a",
   "metadata": {},
   "source": [
    "## 7. Confidence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0111bba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence = percentage of models agreeing with majority\n",
    "def calculate_confidence(lstm, rf, xgb, vote):\n",
    "    if vote == 0:  # Neutral\n",
    "        return 33.33  # Lowest confidence\n",
    "    \n",
    "    agreeing = 0\n",
    "    if lstm == vote:\n",
    "        agreeing += 1\n",
    "    if rf == vote:\n",
    "        agreeing += 1\n",
    "    if xgb == vote:\n",
    "        agreeing += 1\n",
    "    \n",
    "    return (agreeing / 3) * 100\n",
    "\n",
    "ensemble_df['Confidence'] = ensemble_df.apply(\n",
    "    lambda row: calculate_confidence(\n",
    "        row['LSTM_Pred'], row['RF_Pred'], row['XGB_Pred'], row['Ensemble_Vote']\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print('Confidence scores calculated')\n",
    "print(f'Average confidence: {ensemble_df[\\\"Confidence\\\"].mean():.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473717be",
   "metadata": {},
   "source": [
    "## 8. Save Ensemble Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d013bcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output CSV with essential columns\n",
    "output_df = ensemble_df[['Time', 'LSTM_Pred', 'RF_Pred', 'XGB_Pred', 'Ensemble_Vote', 'Confidence']].copy()\n",
    "\n",
    "# Rename for clarity\n",
    "output_df.columns = ['Time', 'LSTM', 'RandomForest', 'XGBoost', 'Ensemble', 'Confidence']\n",
    "\n",
    "output_df.to_csv('ensemble_ha15m_forecast.csv', index=False)\n",
    "\n",
    "print('Ensemble forecast saved: ensemble_ha15m_forecast.csv')\n",
    "print(f'Shape: {output_df.shape}')\n",
    "print(f'\\nFirst 10 rows:')\n",
    "print(output_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7505db74",
   "metadata": {},
   "source": [
    "## 9. Model Agreement Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba313c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check pairwise agreement between models\n",
    "lstm_rf_agree = (ensemble_df['LSTM_Pred'] == ensemble_df['RF_Pred']).sum()\n",
    "lstm_xgb_agree = (ensemble_df['LSTM_Pred'] == ensemble_df['XGB_Pred']).sum()\n",
    "rf_xgb_agree = (ensemble_df['RF_Pred'] == ensemble_df['XGB_Pred']).sum()\n",
    "\n",
    "total = len(ensemble_df)\n",
    "\n",
    "print('Pairwise Agreement:')\n",
    "print(f'  LSTM ↔ RF:   {lstm_rf_agree}/{total} ({lstm_rf_agree/total*100:.1f}%)')\n",
    "print(f'  LSTM ↔ XGB:  {lstm_xgb_agree}/{total} ({lstm_xgb_agree/total*100:.1f}%)')\n",
    "print(f'  RF ↔ XGB:    {rf_xgb_agree}/{total} ({rf_xgb_agree/total*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e1db3b",
   "metadata": {},
   "source": [
    "## 10. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0db1281",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*60)\n",
    "print('ENSEMBLE VOTING SUMMARY')\n",
    "print('='*60)\n",
    "print(f'\\nTotal Bars: {len(ensemble_df)}')\n",
    "print(f'\\nSignal Distribution:')\n",
    "print(f'  Bullish (1):   {bullish_signals:6d} ({bullish_signals/total*100:5.1f}%)')\n",
    "print(f'  Bearish (-1):  {bearish_signals:6d} ({bearish_signals/total*100:5.1f}%)')\n",
    "print(f'  Neutral (0):   {neutral_signals:6d} ({neutral_signals/total*100:5.1f}%)')\n",
    "print(f'\\nConsensus Metrics:')\n",
    "print(f'  Average Consensus: {avg_consensus:.1f}%')\n",
    "print(f'  Average Confidence: {ensemble_df[\\\"Confidence\\\"].mean():.1f}%')\n",
    "print(f'\\nModel Agreement:')\n",
    "print(f'  LSTM ↔ RF:   {lstm_rf_agree/total*100:.1f}%')\n",
    "print(f'  LSTM ↔ XGB:  {lstm_xgb_agree/total*100:.1f}%')\n",
    "print(f'  RF ↔ XGB:    {rf_xgb_agree/total*100:.1f}%')\n",
    "print(f'\\nOutput File: ensemble_ha15m_forecast.csv')\n",
    "print('='*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
